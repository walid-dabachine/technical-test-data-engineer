<!-- Inscrivez vos rÃ©ponses dans ce document -->

## âš™ Instructions d'installation du projet

1. S'assurer que Python est bien installÃ© sur la machine. Ce projet est compatible avec python `>=3.9, <=3.13`. Il a Ã©tÃ© testÃ© en Python 3.10

2. CrÃ©er un environement virtuel nommÃ© `venv` :
    ```shell
    python -m venv venv
    ```

3. Lancer l'environement virtuel :
    ```shell
    source venv/bin/activate
    ```

4. Installer `poetry` pour la gestion des dÃ©pendances du projet :
    ```shell
    pip install poetry && pip install --upgrade pip
    ```

5. Lancer l'installation du projet via `poetry` :
    ```shell
    poetry install
    ````

6. A ce stade, le projet est installÃ© et prÃªt Ã  Ãªtre lancÃ©. CommenÃ§ons par lancer la suite de tests via `pytest` pour s'assurer que tout fonctionne : 
    ```shell
    pytest -rA
    ```
    Un total de 5 tests unitaires doivent passer avec succÃ¨s

## âš¡ Lancement de l'extraction des donnÃ©es

1. S'ouvrir un terminal en arriÃ¨re fond en s'assurant de bien Ãªtre Ã  la racine du projet `technical-test-data-engineer` et en activant l'environement virtuel, puis lancer le serveur API Moovitamix localement : 
    ```shell
    python -m uvicorn src.moovitamix_fastapi.main:app --reload
    ```

2. S'ouvrir Ã  prÃ©sent un nouveau terminal de travail, toujours Ã  la racine du projet `technical-test-data-engineer` et lancer la pipeline d'ingestion, dans un premier temps, sans le module d'orchestration : 
    ```shell
    python src/moovitamix_data_connector/main.py --scheduling no
    ```
    La pipeline va lancer 3 flux d'ingestions: `tracks`, `users` et `listen_history`. Une fois l'exÃ©cution terminÃ©e, les donnÃ©es seront sauvegardÃ©es dans le dossier `data/01_source`

3. Pour activer le module d'orchestration `prefect` : 

    - S'assurer que `prefect` est bien installÃ© en roulant la commande suivante :
        ```shell
        prefect version
        ```
    - Ouvrir un nouveau terminal en arriÃ¨re fond pour lancer le serveur localement :
        ```shell
        prefect server start
        ```
        L'adresse du dashboard local s'affiche, normalement `http://127.0.0.1:4200`
    - Ouvrir un nouveau terminal de travail pour lancer la pipeline d'ingestion avec le module d'orchestration
        ```shell
        python src/moovitamix_data_connector/main.py --scheduling yes
        ```
    - Se rendre sur le dashboard Ã  l'adresse affichÃ©e prÃ©cedemment pour suivre le lancement des runs Ã  chaque minute et leur Ã©tat. Pour l'instant le projet n'implÃ©mente pas de mÃ©thodologie de versioning, les donnÃ©es sont donc extraites et Ã©crasÃ©es Ã  chaque minute sous `src/01_source`.

## ðŸ“– Description des choix techniques du projet

Globalement, le projet se dÃ©coupe en X thÃ©matiques techniques :

1. Setup du projet
2. Connecteur API Moovitamix
3. ModÃ¨le de donnÃ©es
4. Orchestration

Nous allons en explorer les choix, remarques et prochaines Ã©tapes.

### 1. Setup du projet

- Toutes les composantes sont codÃ©es en Python pour faciliter la collaboration avec l'Ã©quipe Data Science qui travaille principalement avec ce language. L'utilisation du SQL notamment pour le module de transformation de donnÃ©es est Ã©galement une option valide Ã  explorer

- J'ai utilisÃ© `poetry` pour la gestion des dÃ©pendances du projet. Sa capacitÃ© Ã  rÃ©soudre les dÃ©pendances automatiquement, centraliser en un seul fichier les configurations de dÃ©v et de prod, centraliser les commandes de build ainsi qu'encapsuler la gestion des dÃ©pendances et de l'environement virtuel en font un outil trÃ¨s robuste en comparaison Ã  une utilisation manuelle de `pip`

- Le projet est dÃ©coupÃ© en 3 modules techniques sous `src` :
    1. `moovitamix_fastapi` : Module de crÃ©ation de l'API (fourni par MoovAI)
    2. `moovitamix_data_connector` : Connecteur de donnÃ©es visant Ã  extraire les donnÃ©es `source` de l'API Moovitamix
    3. `data_transformation` : Module responsable de la crÃ©ation des pipelines de transformation de donnÃ©es de la couche `source` Ã  la couche `mart`

### 2. Connecteur API Moovitamix

- Le connecteur se trouve dans `src/moovitamix_data_connector.py`

- J'ai fait le choix de le coder from scratch bien que d'autres options sur Ã©tagÃ¨re et beaucoup plus robustes existent afin d'Ã©viter de rÃ©inventer la roue :
    * CrÃ©er un connecteur `Airbyte`
    * CrÃ©er un connecteur `Kedro`
    
    Cela s'explique notamment par 2 raisons:
    
    1. un manque de temps pour rentrer dans la doc de ses solutions respectives qui offrent un template prÃªt Ã  l'usage pour crÃ©er des connecteurs personnalisÃ©es mais nÃ©cessite d'adhÃ©rer Ã  leur modÃ¨le de donnÃ©es pour en tirer profit
    
    2. Les connecteurs gÃ©nÃ©riques de type API HTTP de ces 2 solutions n'offrent pas la gestion de la pagination et/ou de gestion du retry en cas de dÃ©passement de la limite de l'API

- Le connecteur gÃ¨re la pagination de l'API

- Le connecteur gÃ¨re pour l'instant uniquement les erreurs de type 429 (rate limit error)

- Le connecteur implÃ©mente un mechanisme de validation de donnÃ©es Ã  l'aide de `Pydantic`. Qui implÃ©mente pour l'instant exclusivement la validation du typage. Une des petites limitations de Pydantic ici est le fait qu'il n'offre pas de solution DataFrame-like permettant d'Ã©valuer la validitÃ© des donnÃ©es au niveau du Dataframe Pandas lors du runtime. Une solution comme `pandera` pourrait Ãªtre une bonne alternative pour amÃ©liorer cela pour Ã©tendre, simplifier et robustifier le scope de la validation

- Prochaines Ã©tapes:
    * PrÃ©sentemment, le connecteur se rÃ©alise systÃ©matiquement une extraction de donnÃ©es en mode "full load". Cela n'est pas trÃ¨s performant ici car pour ce type de donnÃ©es on peut s'imaginer que la volumÃ©trie peut trÃ¨s vite grossir ce qui rendra une extraction de type incrÃ©mentale beaucoup plus performante du point de vue du cout et du temps. On devra donc rÃ©flÃ©chir Ã  l'avenir Ã  l'implÃ©mentation d'une telle feature
    * Explorer l'Ã©ventualitÃ© d'adhÃ©rer Ã  un framework offrant des connecteurs sur-Ã©tagÃ¨re beacoup plus robuste afin d'Ã©viter de rÃ©inventer la roue
    * Etendre la gestion des erreurs Ã  plus de code HTML pour le rendre plus robuste

### 3. ModÃ¨le de donnÃ©es

#### 3.1 Structuration du pipeline de transformation de donnÃ©es

J'ai tirÃ© mon modÃ¨le de donnÃ©es des best practices provenant des frameworks open source utilisÃ©s par la communautÃ©, notamment [dbt](https://docs.getdbt.com/best-practices/how-we-structure/1-guide-overview). La donnÃ©e passe par 3 couches de transformation:

1. Source: DonnÃ©e brute sans transformation
2. Staging: Couche oÃ¹ sont appliquÃ©es les transformations de base de notre modÃ¨le de donnÃ©es (ex: casting, cleaning, ...)
3. Mart: Couche oÃ¹ se trouve les modÃ¨les de donnÃ©es utilisÃ©s par le business, l'Ã©quipe Data Science. Ils dÃ©finissent l'unique source of truth qui permet d'alimenter l'ensemble des cas d'usage analytiques plus bas dans le pipeline (ex: on y dÃ©finit ce qu'est un utilisateur, ce qu'est un track, ...)

Par manque de temps, j'ai eu la possibilitÃ© d'explorer uniquement la couche source qui est la couche oÃ¹ les donnÃ©es extraites par le connecteur API atterissent. Si demain on dÃ©cide d'intÃ©grer une nouvelle source de donnÃ©es comme une base de donnÃ©es SQL, c'est dans cette couche qu'atterisseront aussi les tables de cette BD.

Pour cet exercice, il n'y a pas Ã©normÃ©ment de transformation Ã  appliquer sur les donnÃ©es, les modÃ¨les dÃ©veloppÃ©s dans ces diffÃ©rentes couches auraient Ã©tÃ© lÃ©gers. Si j'avais eu davantage de temps j'aurais utilisÃ© `Kedro` pour rÃ©aliser ces transformations. Sa solution de pipelining et de visualisation des jobs de transformation de donnÃ©es le rend trÃ¨s efficace.

#### 3.2 Structuration du modÃ¨le de donnÃ©es

Etant donnÃ©e la nature du problÃ¨me, j'ai optÃ© pour une modÃ©lisation en Ã©toile (star schema) avec les composantes suivantes:

* `tracks` : table de dimension
* `users` : tables de dimension
* `listen_history` : table de fait

plus d'info dans les [best bractices dbt](https://docs.getdbt.com/terms/dimensional-modeling)

### 4. Orchestration

- J'ai utilisÃ© `Prefect` car moins verbeux qu'une solution comme `Airflow`. Avec le peu de temps que j'ai, j'ai prÃ©fÃ©rÃ© optÃ© pour cette solution pour implÃ©menter rapidement une soltuion d'orchestration. L'inconvÃ©nient d'une solution comme `Prefect` est sa maturitÃ© relativement faible en comparaison Ã  `Airflow` qui est la solution la plus populaire dans l'Ã©cosystÃ¨me Python.

- Ma solution n'implÃ©mente pas pour l'instant de versioning, Ã  chaque run du job les donnÃ©es fraiches viendront Ã©crasÃ©es les anciennes donnÃ©es. Si j'avais plus de temps j'aurais d'abord commencÃ© par analyser les diffÃ©rentes options Ã  ma disposition pour implÃ©menter ce versioning. Cela peut passer par diffÃ©rentes options comme:
    - un format de donnÃ©es qui gÃ¨re by design les versions (ex: delta lake qui est du parquet sous stÃ©roide)
    - une structure de fichier/dossier qui conserve la tracabilitÃ© (ex: un dossier par date, avec un dossier `latest` contenant en tout temps la version la plus Ã  jour des donnÃ©es)